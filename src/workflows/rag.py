# -*- coding: utf-8 -*-
"""RAG workflow using LangGraph.

This workflow implements a Retrieval-Augmented Generation pipeline:
1. Process user query
2. Search documents using Gemini File Search API
3. Build context from search results
4. Generate response with grounding information
"""

from typing import TypedDict, Annotated, Sequence
from langgraph.graph import StateGraph, END


class GroundingChunk(TypedDict):
    """A chunk of grounding information from search results."""

    content: str
    source: str  # file name
    page: int | None  # page number if available


class RAGState(TypedDict):
    """State for RAG workflow.

    Attributes:
        channel_id: The channel (FileSearchStore) ID
        query: User's question
        search_results: Raw search results from Gemini API
        context_chunks: Processed context chunks with source info
        response: Generated response
        grounding_sources: List of sources used in the response
        error: Error message if any step fails
    """

    channel_id: str
    query: str
    search_results: list[dict] | None
    context_chunks: list[GroundingChunk]
    response: str
    grounding_sources: list[str]
    error: str | None


def process_query(state: RAGState) -> RAGState:
    """Process and validate the user query.

    This node:
    - Validates the query is not empty
    - Prepares the query for search
    """
    query = state.get("query", "").strip()

    if not query:
        return {**state, "error": "Query cannot be empty"}

    # Query is valid, continue to search
    return {**state, "query": query, "error": None}


def search_documents(state: RAGState) -> RAGState:
    """Search documents using Gemini File Search API.

    This node:
    - Calls Gemini File Search API with the query
    - Returns raw search results

    Note: Actual API call will be implemented in GeminiService
    """
    # This will be replaced with actual GeminiService call
    # For now, return empty results to allow workflow testing
    if state.get("error"):
        return state

    # Placeholder - will be replaced with actual search
    return {
        **state,
        "search_results": [],
        "context_chunks": [],
    }


def build_context(state: RAGState) -> RAGState:
    """Build context from search results.

    This node:
    - Extracts relevant chunks from search results
    - Formats context with source information
    - Prepares grounding data
    """
    if state.get("error"):
        return state

    search_results = state.get("search_results", [])

    if not search_results:
        return {
            **state,
            "context_chunks": [],
            "grounding_sources": [],
        }

    # Process search results into context chunks
    context_chunks: list[GroundingChunk] = []
    sources: set[str] = set()

    for result in search_results:
        chunk: GroundingChunk = {
            "content": result.get("content", ""),
            "source": result.get("source", "unknown"),
            "page": result.get("page"),
        }
        context_chunks.append(chunk)
        sources.add(chunk["source"])

    return {
        **state,
        "context_chunks": context_chunks,
        "grounding_sources": list(sources),
    }


def generate_response(state: RAGState) -> RAGState:
    """Generate response using LLM with context.

    This node:
    - Builds prompt with context
    - Calls LLM to generate response
    - Includes grounding information in response
    """
    if state.get("error"):
        return state

    context_chunks = state.get("context_chunks", [])
    query = state.get("query", "")

    if not context_chunks:
        return {
            **state,
            "response": "No relevant documents found for your query.",
        }

    # Build context string
    context_parts = []
    for i, chunk in enumerate(context_chunks, 1):
        source_info = f"[Source: {chunk['source']}"
        if chunk.get("page"):
            source_info += f", Page {chunk['page']}"
        source_info += "]"
        context_parts.append(f"{source_info}\n{chunk['content']}")

    context = "\n\n".join(context_parts)

    # Placeholder response - will be replaced with actual LLM call
    response = f"Based on the documents, here is the answer to: {query}\n\n[Response will be generated by LLM]"

    return {**state, "response": response}


def should_continue(state: RAGState) -> str:
    """Determine if workflow should continue or end due to error."""
    if state.get("error"):
        return "end"
    return "continue"


def create_rag_workflow() -> StateGraph:
    """Create the RAG workflow graph.

    Workflow:
        process_query -> search_documents -> build_context -> generate_response -> END

    Returns:
        Compiled LangGraph workflow
    """
    # Create the graph
    workflow = StateGraph(RAGState)

    # Add nodes
    workflow.add_node("process_query", process_query)
    workflow.add_node("search_documents", search_documents)
    workflow.add_node("build_context", build_context)
    workflow.add_node("generate_response", generate_response)

    # Set entry point
    workflow.set_entry_point("process_query")

    # Add edges with conditional routing
    workflow.add_conditional_edges(
        "process_query",
        should_continue,
        {
            "continue": "search_documents",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "search_documents",
        should_continue,
        {
            "continue": "build_context",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "build_context",
        should_continue,
        {
            "continue": "generate_response",
            "end": END,
        },
    )

    workflow.add_edge("generate_response", END)

    return workflow.compile()


# Create a singleton instance for easy import
rag_workflow = create_rag_workflow()
